{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05d9b563-8d20-4071-9039-6aa7d2790969",
   "metadata": {},
   "source": [
    "### Text PreProcessing\n",
    "\n",
    "To transform words into numerical form that can work with machine learning algorithms we use text preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fd3b36-35c0-4901-b464-fa40f6ef5ac7",
   "metadata": {},
   "source": [
    "### Text Preprocessing Steps for NLP\n",
    "Text preprocessing is crucial for preparing raw text data for NLP tasks. Below are common steps in a typical preprocessing pipeline:\n",
    "\n",
    "1. Lowercasing:\n",
    "Convert all text to lowercase to reduce the complexity of comparisons.\n",
    "Example:\n",
    "\n",
    "- Input: \"Hello World!\"\n",
    "- Output: \"hello world!\"\n",
    "\n",
    "2. Tokenization:\n",
    "Split the text into individual words or sentences.\n",
    "Example:\n",
    "\n",
    "- Input: \"NLP is amazing!\"\n",
    "- Output: [\"NLP\", \"is\", \"amazing\", \"!\"]\n",
    "\n",
    "3. Removing Punctuation:\n",
    "Remove special characters and punctuation to clean the text.\n",
    "Example:\n",
    "\n",
    "- Input: \"Hello, NLP World!\"\n",
    "- Output: \"Hello NLP World\"\n",
    "\n",
    "4. Stopword Removal:\n",
    "Remove common words (like \"the,\" \"is\") that do not add meaningful information.\n",
    "Example:\n",
    "\n",
    "- Input: \"This is a great book!\"\n",
    "- Output: [\"great\", \"book\"]\n",
    "\n",
    "5. Stemming:\n",
    "Reduce words to their root form (may not be linguistically accurate).\n",
    "Example:\n",
    "\n",
    "- Input: \"playing, played, plays\"\n",
    "- Output: \"play\"\n",
    "\n",
    "6. Lemmatization:\n",
    "Get the base form of a word using linguistic context (better than stemming).\n",
    "Example:\n",
    "\n",
    "- Input: \"running, ran, runs\"\n",
    "- Output: \"run\"\n",
    "\n",
    "7. Removing Numerical Values (Optional): Remove numbers if they are not useful for the task.\n",
    "\n",
    "8. Handling Contractions: Expand common contractions (e.g., \"can't\" → \"cannot\").\n",
    "\n",
    "\n",
    "9. Removing Extra Whitespaces: Clean up multiple spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f35490c3-4b91-4d66-8f23-7fa9265ef65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48632037-a9c1-4316-9e0c-aad4fd5048a1",
   "metadata": {},
   "source": [
    "### Lowercasing: \n",
    "Convert all text to lowercase to reduce the complexity of comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9874b0e9-22ac-4450-b181-3ab3fbfe38fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_text(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98d4303b-5770-42f0-ab02-c0c086cee266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the world is round'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"The World is Round\"\n",
    "lowercase_text(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbef26e-b157-4093-b559-8f229b30c418",
   "metadata": {},
   "source": [
    "### Tokenization:\n",
    "Split the text into individual words or sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b7bf08cf-f5d0-421b-8a00-9ed3db74b6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\sharm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> import nltk\n",
    ">>> nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "98002e6e-ef0e-46a9-b9ff-f9171b701c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "645e77ee-330c-4b12-8eec-e6e1761b7748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today',\n",
       " 'the',\n",
       " 'sky',\n",
       " 'is',\n",
       " 'cloudy',\n",
       " 'and',\n",
       " 'winds',\n",
       " 'are',\n",
       " 'strong',\n",
       " '.',\n",
       " 'There',\n",
       " 'are',\n",
       " 'chances',\n",
       " 'of',\n",
       " 'heavy',\n",
       " 'rain',\n",
       " 'and',\n",
       " 'storm',\n",
       " '.']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \" Today the sky is cloudy and winds are strong. There are chances of heavy rain and storm.\"\n",
    "tokenize_text(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becb6652-62a2-42e4-be80-099435f9d7a5",
   "metadata": {},
   "source": [
    "### Removing Punctuation: \n",
    "Remove special characters and punctuation to clean the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ec745b01-a93a-40c4-8a71-07f45433f2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def rem_punc(text):\n",
    "    translator = str.maketrans('','', string.punctuation)\n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "faf2195a-9114-4082-b9f4-1d97475086bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A parakeet is any one of many small to mediumsized species of parrot in multiple genera that generally has long tail feathers Isnt this fact exciting'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_data = \"A parakeet!! is any one of many small to medium-sized species of parrot, in multiple genera, that generally has long tail~ feathers. Isn't this fact exciting???\"  \n",
    "rem_punc(string_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dde823-85ce-4b5d-bd4c-6dc7d543f96b",
   "metadata": {},
   "source": [
    "### Stopword Removal:\n",
    "Remove common words (like \"the,\" \"is\") that do not add meaningful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a5f7f419-7560-4152-9822-678d0e217987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def rem_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words_token = word_tokenize(text)\n",
    "    filtered_text = [ word for word in words_token if word not in string.punctuation]\n",
    "    filtered_text = [ word for word in words_token if word not in stop_words]\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a345696b-f470-4f51-9967-6a827fc289e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AI', 'might', 'last', 'intelligent', 'invention', 'humanity', '.']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_text = \" AI might be the last and most intelligent invention of humanity. \"\n",
    "rem_stopwords(string_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9ad875-e17b-40a9-9fc6-c22966d61d72",
   "metadata": {},
   "source": [
    "### Stemming:\n",
    "Reduce words to their root form (may not be linguistically accurate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a60652b3-c4f2-4fcc-a97c-2b5c9d28f182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "root = PorterStemmer()\n",
    "\n",
    "def root_words(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = [root.stem(word) for word in tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8b160380-2f0a-4a35-b8bf-bdd6f3f7d0a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['some',\n",
       " 'peopl',\n",
       " 'believ',\n",
       " 'that',\n",
       " 'artifici',\n",
       " 'intellig',\n",
       " 'will',\n",
       " 'becom',\n",
       " 'so',\n",
       " 'advanc',\n",
       " 'that',\n",
       " 'it',\n",
       " 'will',\n",
       " 'surpass',\n",
       " 'human',\n",
       " 'intellig',\n",
       " 'and',\n",
       " 'effect',\n",
       " 'take',\n",
       " 'control',\n",
       " 'of',\n",
       " 'the',\n",
       " 'planet']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_text = \" Some people believe that artificial intelligence will become so advanced that it will surpass human intelligence and effectively take control of the planet\"\n",
    "root_words(string_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cbe7bb-2c8a-4d80-afa6-39a1f5e7359f",
   "metadata": {},
   "source": [
    "### Lemmatization: \n",
    "Get the base form of a word using linguistic context (better than stemming)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ba703a64-60bf-4186-b9ac-9c8ea99a84c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sharm\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> import nltk\n",
    ">>> nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d60f31eb-8fb3-439d-9461-a37842490892",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemma = wordnet.WordNetLemmatizer()\n",
    "\n",
    "def lem_words(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lem_text = [lemma.lemmatize(word, pos = 'v') for word in tokens]\n",
    "    return lem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2175fb2a-aa89-4ad4-92d1-d4c8af5a3ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Some',\n",
       " 'people',\n",
       " 'believe',\n",
       " 'that',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'will',\n",
       " 'become',\n",
       " 'so',\n",
       " 'advance',\n",
       " 'that',\n",
       " 'it',\n",
       " 'will',\n",
       " 'surpass',\n",
       " 'human',\n",
       " 'intelligence',\n",
       " 'and',\n",
       " 'effectively',\n",
       " 'take',\n",
       " 'control',\n",
       " 'of',\n",
       " 'the',\n",
       " 'planet']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_text = \" Some people believe that artificial intelligence will become so advanced that it will surpass human intelligence and effectively take control of the planet\"\n",
    "lem_words(string_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8e1417-afdf-4e8a-8a9b-c78ce9957ec1",
   "metadata": {},
   "source": [
    "### Removing Numerical Values (Optional):\n",
    "Remove numbers if they are not useful for the task.\n",
    "\n",
    "Use a module like inflect or num2words to convert numbers to words.\n",
    "\n",
    "1. Syntax:\n",
    "\n",
    "- num2words(string)\n",
    "\n",
    "2. Syntax:\n",
    "\n",
    "- p = inflect.engine()\n",
    "- word_representation = p.number_to_words(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "67c1b9f3-7008-4670-a1d8-838b2996dd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect\n",
    "\n",
    "# Initialize the engine\n",
    "q = inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0295dc35-f850-4704-9b4c-e00f7317a1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_num(text):\n",
    "    string = text.split()\n",
    "    new_str = []                           # Initialize empty list\n",
    "\n",
    "    for word in string:\n",
    "\n",
    "        if word.isdigit():\n",
    "            temp = q.number_to_words(word) # Store the converted digit in temp variable\n",
    "            new_str.append(temp)           # Append to new_str list\n",
    "        else:\n",
    "            new_str.append(word)           # Append text to new_str list\n",
    "\n",
    "    string = \" \".join(new_str)             # Join text of new-str list\n",
    "    return string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "831e5300-f98b-4a60-b1b9-a3b06476f788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I need to buy one dozen mangos, five kg potatoes, one hundred gm chilli powder from the grocery store.'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_text = \"I need to buy 1 dozen mangos, 5 kg potatoes, 100 gm chilli powder from the grocery store.\"\n",
    "convert_num(string_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0966f33-16c2-4a90-bdb0-3dc3fffb544a",
   "metadata": {},
   "source": [
    "### Handling Contractions:\n",
    "Expand common contractions (e.g., \"can't\" → \"cannot\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6a307f2e-feaa-4202-a064-a47903c21bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "def expand_contractions(text):\n",
    "    expanded_words = [contractions.fix(word) for word in text.split()]\n",
    "    return \" \".join(expanded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8f5e5b17-e066-4722-937d-a25513246bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I cannot go there, it is too late.'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_text = \"I can't go there, it's too late.\"\n",
    "expand_contractions(string_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b122a516-b352-44f7-927c-51446e75d0a7",
   "metadata": {},
   "source": [
    "### Removing Extra Whitespaces: \n",
    "Clean up multiple spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b5a1d793-0254-414e-8030-61f165b7b143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_whitespaces(text):\n",
    "    return \" \".join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "eebca70a-81c3-4953-90c9-8508632ccd5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is an example with extra spaces.'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"    This   is    an   example    with   extra spaces.\"\n",
    "remove_extra_whitespaces(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02392c19-be0d-423b-bc20-3cf4334da843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
